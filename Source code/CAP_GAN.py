# -*- coding: utf-8 -*-
'''
Note:
Experiment results for Cells and CIFAR-10 are generated by this CAP_GAN code.

'''

import os
import random
import cv2
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import tensorflow.keras.backend as K
from tensorflow.keras import Model, Sequential
from tensorflow.keras.initializers import RandomNormal
from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \
    Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \
    Concatenate, multiply, Flatten, BatchNormalization
from tensorflow.keras.initializers import glorot_normal
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from IPython import display

import glob
import imageio
import matplotlib.pyplot as plt
import numpy as np
import keras.backend as K
import PIL
import tensorflow as tf
import tensorflow_probability as tfp
import time
from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, Activation, LeakyReLU, LayerNormalization, BatchNormalization

"""Author: Yuchong Yao

Create Imbalance Data
"""

random.seed(2021)

from tensorflow.keras.datasets.cifar10 import load_data

# Load training set
(images, labels), (_,_) = load_data()
labels = labels.reshape(-1)

# Sample classes
ratio=10

# Define the majority class
majority = 8

train_images = []
train_labels = []
for i in range(np.unique(labels).shape[0]):
    cur_images = images[labels == i]
    # divide ratio for minority
    if i != majority:
        # can shuffle for random choice
        # np.random.shuffle(cur_images)
        cur_images = cur_images[:(cur_images.shape[0])//ratio]
    train_images += list(cur_images)
    train_labels += [i]*cur_images.shape[0]
images = np.array(train_images)
labels = np.array(train_labels)
shuffler = np.random.permutation(len(images))
images = images[shuffler]
labels = labels[shuffler]

print(images.shape)
print(labels.shape)
print(np.unique(labels))

"""Random Over Sampling"""

majority_size = len(np.where(labels == majority)[0])
resample_train_images = []
resample_train_labels = []
for c in np.unique(labels):
    c_images = images[labels == c]
    c_labels = labels[labels == c] 
    if c != majority:  
        ids = np.arange(len(c_images))
        choices = np.random.choice(ids, majority_size)
        c_images = c_images[choices,]
        c_labels = c_labels[choices,]
    resample_train_images += list(c_images)
    resample_train_labels += list(c_labels)
        

resample_train_images = np.array(resample_train_images)
resample_train_labels = np.array(resample_train_labels)
shuffler = np.random.permutation(len(resample_train_images))
resample_train_images = resample_train_images[shuffler]
resample_train_labels = resample_train_labels[shuffler]  
print(resample_train_images.shape, resample_train_labels.shape)

"""Models Setup"""

(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()
def preprocess_images(images):
  images = images.reshape((images.shape[0], 32, 32, 3)) / 255. # [0, 1]
#   images = (images.reshape((images.shape[0], 32, 32, 3)).astype('float32') - 127.5) / 127.5 # [-1, 1]
  return images.astype('float32')

train_images = preprocess_images(train_images)
test_images = preprocess_images(test_images)
train_size = 60000
batch_size = 32
test_size = 10000
train_dataset = tf.data.Dataset.from_tensor_slices(train_images).batch(batch_size)
train_label = tf.data.Dataset.from_tensor_slices(train_labels).batch(batch_size)
test_dataset = tf.data.Dataset.from_tensor_slices(test_images).batch(batch_size)
test_label = tf.data.Dataset.from_tensor_slices(test_labels).batch(batch_size)

class Reparameterize(tf.keras.layers.Layer):
    def call(self, inputs):
  
        mu, sigma = inputs

        batch = tf.shape(mu)[0]
        dim = tf.shape(mu)[1]

        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return mu + tf.exp(0.5 * sigma) * epsilon

def Encoder(input_shape, latent_dim):

 
    inputs = tf.keras.layers.Input(shape=input_shape)


    x = tf.keras.layers.Conv2D(filters=64, kernel_size=4, strides=2, padding="same", activation=LeakyReLU(), name="encode_conv1")(inputs)
    # x = tf.keras.layers.BatchNormalization()(x) ## ?
    x = tf.keras.layers.LayerNormalization()(x)
    x = tf.keras.layers.Conv2D(filters=128, kernel_size=4, strides=2, padding='same', activation=LeakyReLU(), name="encode_conv2")(x)
    # x = tf.keras.layers.BatchNormalization()(x) ## ?
    x = tf.keras.layers.LayerNormalization()(x)
    x = tf.keras.layers.Conv2D(filters=128, kernel_size=4, strides=2, padding='same', activation=LeakyReLU(), name="encode_conv3")(x)
    # x = tf.keras.layers.BatchNormalization()(x) ## ?
    x = tf.keras.layers.LayerNormalization()(x)
    x = tf.keras.layers.Conv2D(filters=256, kernel_size=4, strides=2, padding='same', activation=LeakyReLU(), name="encode_conv4")(x)
    # x = tf.keras.layers.BatchNormalization()(x) ## ?
    x = tf.keras.layers.LayerNormalization()(x)
    conv_shape = x.shape


    x = tf.keras.layers.Flatten(name="encode_flatten")(x)
    mu = tf.keras.layers.Dense(latent_dim, name='latent_mu')(x)
    sigma = tf.keras.layers.Dense(latent_dim, name ='latent_sigma')(x)


    z = Reparameterize()((mu, sigma))

    model = tf.keras.Model(inputs, outputs=[mu, sigma, z])
    return model, conv_shape

def Embedder(latent_dim, n_classes):

    label = tf.keras.layers.Input((1,), dtype='int32')
    noise = tf.keras.layers.Input(shape=(latent_dim,))
    # ne = Dense(256)(noise)
    # ne = LeakyReLU(0.2)(ne)

  

    embedding = tf.keras.layers.Flatten()(tf.keras.layers.Embedding(n_classes, latent_dim)(label))
    # le = Dense(256)(le)
    # le = LeakyReLU(0.2)(le)

    embedding = tf.keras.layers.multiply([noise, embedding])
    # noise_le = Dense(latent_dim)(noise_le)

    model = tf.keras.Model(inputs=[noise, label], outputs=embedding)

    return model

def Decoder(latent_dim, conv_shape, n_channel):

    inputs = tf.keras.layers.Input(shape=(latent_dim,))

    units = conv_shape[1] * conv_shape[2] * conv_shape[3]
    x = tf.keras.layers.Dense(units, activation = LeakyReLU(), name="decode_dense1")(inputs)
    x = tf.keras.layers.BatchNormalization()(x) ## ?

    x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]), name="decode_reshape")(x)


    x = tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=4, strides=2, padding='same', activation=LeakyReLU(), name="decode_conv2d_1")(x)
    # x = tf.keras.layers.BatchNormalization()(x) ## ?
    x = tf.keras.layers.LayerNormalization()(x)
    x = tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=4, strides=2, padding='same', activation=LeakyReLU(), name="decode_conv2d_2")(x)
    # x = tf.keras.layers.BatchNormalization()(x) ## ?
    x = tf.keras.layers.LayerNormalization()(x)
    x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=4, strides=2, padding='same', activation=LeakyReLU(), name="decode_conv2d_3")(x)
    # x = tf.keras.layers.BatchNormalization()(x) ## ?
    x = tf.keras.layers.LayerNormalization()(x)
    outputs = tf.keras.layers.Conv2DTranspose(filters=n_channel, kernel_size=4, strides=2, padding='same', activation='sigmoid', name="decode_final")(x)

    model = tf.keras.Model(inputs, outputs)
    return model

def kl_reconstruction_loss(inputs, outputs, mu, sigma):

    kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)
    kl_loss = tf.reduce_mean(kl_loss) * -0.5
    return kl_loss

def VAE(encoder, embedder, decoder, input_shape):
    
    inputs = tf.keras.layers.Input(shape=input_shape)
    labels = tf.keras.layers.Input((1,), dtype='int32')

    mu, sigma, z = encoder(inputs)
    labeled_z = embedder([z, labels])
    reconstructed = decoder(labeled_z)
    model = tf.keras.Model(inputs=[inputs, labels], outputs=reconstructed)
    kl = kl_reconstruction_loss(inputs, labeled_z, mu, sigma)
    model.add_loss(kl)
    return model

def get_models(input_shape, latent_dim, n_classes):

    encoder, conv_shape = Encoder(latent_dim=latent_dim, input_shape=input_shape)
    embedder = Embedder(latent_dim=latent_dim, n_classes=n_classes)
    decoder = Decoder(latent_dim=latent_dim, conv_shape=conv_shape, n_channel=input_shape[2])
    vae = VAE(encoder, embedder, decoder, input_shape=input_shape)
    return encoder, embedder, decoder, vae

test_sample = test_images[:10]
test_label_sample = test_labels[:10]
def generate_and_save_images(model, epoch, test_sample, test_label_sample):
    decoded_imgs = model([test_sample, test_label_sample])
    n = 10
    plt.figure(figsize=(20, 4))
    for i in range(n):
        
    # display original
        ax = plt.subplot(2, n, i + 1)
    #   plt.imshow(test_images[i].reshape(28,28))
        plt.imshow(test_sample[i])
        # plt.imshow(test_sample[i]*0.5+0.5)
        plt.title("original")
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)

    # display reconstruction
        ax = plt.subplot(2, n, i + 1 + n)
#   plt.imshow(decoded_imgs[i].reshape(28,28))
        plt.imshow(decoded_imgs[i]) # [0, 1]
        # plt.imshow(decoded_imgs[i]*0.5 + 0.5) # [-1, 1]
        plt.title("reconstructed")
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
    plt.show()

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.5, beta_2=0.9)
loss_metric = tf.keras.metrics.Mean()
ce_loss = tf.keras.losses.BinaryCrossentropy()
mse_loss = tf.keras.losses.MeanSquaredError()
encoder, embedder, decoder, vae = get_models(input_shape=(32,32,3), latent_dim=64, n_classes=10)

def compute_loss(model, x, y):
    
    reconstructed = model([x, y])
    kl = sum(model.losses)
    mse = mse_loss(tf.reshape(x, shape=[-1]), tf.reshape(reconstructed, shape=[-1])) * x.shape[0] * x.shape[1]
    # ce = ce_loss(tf.reshape(x, shape=[-1]), tf.reshape(reconstructed, shape=[-1])) * x.shape[0] * x.shape[1]
    ce = tf.nn.sigmoid_cross_entropy_with_logits(tf.reshape(x, shape=[-1]), tf.reshape(reconstructed, shape=[-1])) * x.shape[0] * x.shape[1]
    return  kl + ce + mse

@tf.function
def train_step(model, x, y, optimizer):

    with tf.GradientTape() as tape:
        loss = compute_loss(model, x, y)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    
epochs = 100
generate_and_save_images(vae, 0, test_sample, test_label_sample)
for epoch in range(1, epochs + 1):
    start_time = time.time()
    for train_x, train_y in zip(train_dataset, train_label):
        train_step(vae, train_x, train_y, optimizer)
        end_time = time.time()

    loss = tf.keras.metrics.Mean()
    for test_x, test_y in zip(test_dataset, test_label):
        loss(compute_loss(vae, test_x, test_y))
    l = loss.result()
    display.clear_output(wait=False)
    print('Epoch: {}, Test set loss: {}, time elapse for current epoch: {}'.format(epoch, l, end_time - start_time))
    generate_and_save_images(vae, epoch, test_sample, test_label_sample)

# Refer to the WGAN-GP Architecture. https://github.com/keras-team/keras-io/blob/master/examples/generative/wgan_gp.py
class CAP_VAE(Model):
    def __init__(
        self,
        discriminator,
        generator,
        latent_dim,
        discriminator_extra_steps=3,
        gp_weight=10.0,
    ):
        super(CAP_VAE, self).__init__()
        self.discriminator = discriminator
        self.generator = generator
        self.latent_dim = latent_dim
        self.train_ratio = trainRatio
        self.gp_weight = gp_weight

    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):
        super(CAP_VAE, self).compile()
        self.d_optimizer = d_optimizer
        self.g_optimizer = g_optimizer
        self.d_loss_fn = d_loss_fn
        self.g_loss_fn = g_loss_fn

    def gradient_penalty(self, batch_size, real_images, fake_images, labels):
        """ Calculates the gradient penalty.
        This loss is calculated on an interpolated image
        and added to the discriminator loss.
        """
        # get the interplated image
        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)
        diff = fake_images - real_images
        interpolated = real_images + alpha * diff

        with tf.GradientTape() as gp_tape:
            gp_tape.watch(interpolated)
            # 1. Get the discriminator output for this interpolated image.
            pred = self.discriminator([interpolated, labels], training=True)

        # 2. Calculate the gradients w.r.t to this interpolated image.
        grads = gp_tape.gradient(pred, [interpolated])[0]
        # 3. Calcuate the norm of the gradients
        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))
        gp = tf.reduce_mean((norm - 1.0) ** 2)
        return gp

    def train_step(self, data):
        if isinstance(data, tuple):
            real_images = data[0]
            labels = data[1]

        # Get the batch size
        batch_size = tf.shape(real_images)[0]

        ########################### Train the Discriminator ###########################
        # For each batch, we are going to perform cwgan-like process
        for i in range(self.train_ratio):
            # Get the latent vector
            random_latent_vectors = tf.random.normal(
                shape=(batch_size, self.latent_dim)
            )
            fake_labels = tf.random.uniform((batch_size,), 0, n_classes)
            wrong_labels = tf.random.uniform((batch_size,), 0, n_classes)
            with tf.GradientTape() as tape:
                # Generate fake images from the latent vector
                fake_images = self.generator([random_latent_vectors, fake_labels], training=True)
                # Get the logits for the fake images
                fake_logits = self.discriminator([fake_images, fake_labels], training=True)
                # Get the logits for real images
                real_logits = self.discriminator([real_images, labels], training=True)
                # Get the logits for wrong label classification
                wrong_label_logits = self.discriminator([real_images, wrong_labels], training=True)

                # Calculate discriminator loss using fake and real logits
                d_cost = self.d_loss_fn(real_logits=real_logits, fake_logits=fake_logits,
                                        wrong_label_logits=wrong_label_logits
                                        )

                # Calculate the gradient penalty
                gp = self.gradient_penalty(batch_size, real_images, fake_images, labels)
                # Add the gradient penalty to the original discriminator loss
                d_loss = d_cost + gp * self.gp_weight

            # Get the gradients w.r.t the discriminator loss
            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)
            # Update the weights of the discriminator using the discriminator optimizer
            self.d_optimizer.apply_gradients(
                zip(d_gradient, self.discriminator.trainable_variables)
            )

        ########################### Train the Generator ###########################
        # Get the latent vector
        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))
        fake_labels = tf.random.uniform((batch_size,), 0, n_classes)
        with tf.GradientTape() as tape:
            # Generate fake images using the generator
            generated_images = self.generator([random_latent_vectors, fake_labels], training=True)
            # Get the discriminator logits for fake images
            gen_img_logits = self.discriminator([generated_images, fake_labels], training=True)
            # Calculate the generator loss
            g_loss = self.g_loss_fn(gen_img_logits)

        # Get the gradients w.r.t the generator loss
        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)
        # Update the weights of the generator using the generator optimizer
        self.g_optimizer.apply_gradients(
            zip(gen_gradient, self.generator.trainable_variables)
        )
        return {"d_loss": d_loss, "g_loss": g_loss}

# Optimizer for both the networks
# learning_rate=0.0002, beta_1=0.5, beta_2=0.9 are recommended
generator_optimizer = Adam(
    learning_rate=0.0002, beta_1=0.5, beta_2=0.9
)
discriminator_optimizer = Adam(
    learning_rate=0.0002, beta_1=0.5, beta_2=0.9
)


# We refer to the DRAGAN loss function. https://github.com/kodalinaveen3/DRAGAN
# Define the loss functions to be used for discrimiator
# We will add the gradient penalty later to this loss function
def discriminator_loss(real_logits, fake_logits, wrong_label_logits):
    real_loss = tf.reduce_mean(
        tf.nn.sigmoid_cross_entropy_with_logits(logits=real_logits, labels=tf.ones_like(real_logits)))
    fake_loss = tf.reduce_mean(
        tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.zeros_like(fake_logits)))
    wrong_label_loss = tf.reduce_mean(
        tf.nn.sigmoid_cross_entropy_with_logits(logits=wrong_label_logits, labels=tf.zeros_like(fake_logits)))

    return wrong_label_loss + fake_loss + real_loss

# Define the loss functions to be used for generator
def generator_loss(fake_logits):
    fake_loss = tf.reduce_mean(
        tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.ones_like(fake_logits)))
    return fake_loss

def generator_label(embedder, decoder, latent_dim):
    

    label = Input((1,), dtype='int32')
    latent = Input((latent_dim,))

    labeled_latent = embedder([latent, label])
    gen_img = decoder(labeled_latent)
    model = Model([latent, label], gen_img)

    return model


def build_discriminator(encoder, img_size, n_classes):

    label = Input((1,), dtype='int32')
    img = Input(img_size)

    inter_output_model = Model(inputs=encoder.input, outputs=encoder.layers[-3].output)
    x = inter_output_model(img)

    le = Flatten()(tf.keras.layers.Embedding(n_classes, 512)(label))
    le = Dense(64)(le)
    le = LeakyReLU(0.2)(le)
    x_y = multiply([x, le])
    x_y = Dense(512)(x_y)

    out = Dense(1)(x_y)
    model = Model(inputs=[img, label], outputs=out)

    return model

"""Compile Models"""

d_model = build_discriminator(encoder, (32,32,3), 10)  
g_model = generator_label(embedder, decoder, 64)  

latent_dim=64
trainRatio=12
n_classes=10

cap_gan = CAP_VAE(
    discriminator=d_model,
    generator=g_model,
    latent_dim=latent_dim,
    discriminator_extra_steps=5,
)

# Compile the model
cap_gan.compile(
    d_optimizer=discriminator_optimizer,
    g_optimizer=generator_optimizer,
    g_loss_fn=generator_loss,
    d_loss_fn=discriminator_loss,
)

"""plot during training"""

# Plot/save generated images through training
channel=3
def plt_img(generator, epoch):
    np.random.seed(42)
    latent_gen = np.random.normal(size=(n_classes, latent_dim))

    n = n_classes

    plt.figure(figsize=(2*n, 2*(n+1)))
    for i in range(n):
        # display original
        ax = plt.subplot(n+1, n, i + 1)
        if channel == 3:
            plt.imshow(test_images[np.where(test_labels==i)[0]][4])
            
        else:
            plt.imshow(test_images[np.where(test_labels==i)[0]][4].reshape(64, 64))
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
        for c in range(n):
            decoded_imgs = generator.predict([latent_gen, np.ones(n)*c])
            # decoded_imgs = decoded_imgs * 0.5 + 0.5
            # display generation
            ax = plt.subplot(n+1, n, (i+1)*n + 1 + c)
            if channel == 3:
                plt.imshow(decoded_imgs[i])
            else:
                plt.imshow(decoded_imgs[i].reshape(64, 64))
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
    plt.savefig('cap_gan_results/generated_plot_%d.png' % epoch)
    plt.show()
    return

# make directory to store results
os.system('mkdir -p cap_gan_results')

# Record the loss
d_loss_history = []
g_loss_history = []

"""Start training"""

LEARNING_STEPS = 50
for learning_step in range(LEARNING_STEPS):
    print('LEARNING STEP # ', learning_step + 1, '-' * 50)
    cap_gan.fit(train_images, train_labels, batch_size=128, epochs=2)
    d_loss_history += cap_gan.history.history['d_loss']
    g_loss_history += cap_gan.history.history['g_loss']
    if (learning_step+1)%1 == 0:
        plt_img(cap_gan.generator, learning_step)

"""Display performance"""

# plot loss of G and D
plt.plot(d_loss_history, label='D')
plt.plot(g_loss_history, label='G')
plt.legend()
plt.show()

# save gif
import imageio
ims = []
for i in range(LEARNING_STEPS):
    fname = 'generated_plot_%d.png' % i
    dir = 'cap_gan_results/'
    if fname in os.listdir(dir):
        print('loading png...', i)
        im = imageio.imread(dir + fname, 'png')
        ims.append(im)
print('saving as gif...')
imageio.mimsave(dir + 'training_demo.gif', ims, fps=3)

"""generate image samples using trained generator"""

# will generate (n_classes * n_classes * steps) = (4 * steps) images, (2 * steps) for each class
# for exapmple you want to generate samples with the same size as MNIST, pass the output_dim = [28, 28]
def generate_images_binary(n_classes, latent_dim, x_test, generator, steps, output_dim,**kwargs):
    np.random.seed(42)
    majority = []
    minority = []
    x_real = x_test * 0.5 + 0.5

    for i in range(steps):
        for c in range(n_classes):
            latent_gen = np.random.normal(size=(n_classes, latent_dim))
            decoded_imgs = generator.predict([latent_gen, np.ones(n)*c])
            decoded_imgs = decoded_imgs * 0.5 + 0.5
            decoded_imgs = tf.image.resize(decoded_imgs, output_dim).numpy()
            
            if c%2:
                minority += list(decoded_imgs)
            else:
                majority += list(decoded_imgs)
    return np.array(majority), np.array(minority)

majority, minority = generate_images_binary(n_classes=n_classes, latent_dim=latent_dim, x_test=x_test, generator=cap_gan.generator, steps=2500, output_dim=[28,28])

print(majority.shape, minority.shape)

np.save("CIFAR10-5-Majority", majority)
np.save("CIFAR10-5-Minority", minority)

def calculate_ssim(real_majority, real_minority, gen_majority, gen_minority, steps = 5):
    
    # resize the number of real majority and minority
    # make sure the size of real image and generated image are same
    real_maj_idx = np.random.choice(real_majority.shape[0], gen_majority.shape[0])
    real_majority = real_majority[real_maj_idx].astype('float32') / 255.
    
    real_min_idx = np.random.choice(real_minority.shape[0], gen_minority.shape[0])
    real_minority = real_minority[real_min_idx].astype('float32') / 255.
    
    
    majority_score = 0
    minority_score = 0
    
    # multiple calculation of the diffrent real images and generated images
    for i in range(steps):
        # random the generated images and real images for majority
        np.random.shuffle(real_majority)
        np.random.shuffle(gen_majority)
        
        # calculate the SSIM of Majority
        maj_ssim = tf.image.ssim(real_majority, gen_majority, max_val=1.0)
        majority_score += np.average(maj_ssim.numpy())
        
        # random the generated images and real images for minority
        np.random.shuffle(real_minority)
        np.random.shuffle(gen_minority)
        
        # calculate the SSIM of Minority
        min_ssim = tf.image.ssim(real_minority, gen_minority, max_val=1.0)
        minority_score +=np.average(min_ssim.numpy())
    
    majority_score /= steps
    minority_score /= steps
    
    print('>>SSIM(0): %.3f' % majority_score)
    print('>>SSIM(1): %.3f' % minority_score)
    
    return majority_score, minority_score
